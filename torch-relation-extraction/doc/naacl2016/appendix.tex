\section{Appendix}

\subsection{Additional Qualitative Results \label{sec:more-qual-anal}}

% cross lingual relations
Qualitative analysis of our multilingual models further suggests that they successfully embed semantically similar relations across languages using tied entity pairs and translation dictionary as grounding. Table \ref{tab:cross-lingual-relations} lists three top nearest neighbors in English for several Spanish patterns from the text. In each case, the English patterns capture the relation represented in the Spanish text.

\newcommand{\tablespace}{\end{tabular}
\newline
\newline
%\hspace*{-21pt}
\begin{tabular}{|p{7.6cm}|}
}
\begin{table}[h]
\begin{center}
\small
%\hspace*{-21pt}
\begin{tabular}{|p{7.6cm}|}
\hline
 \bf y cuatro de sus familias, incluidos su esposa, \endgraf \hspace{5pt}Wu Shu-chen, su hijo, \\
\it{ and four of his family members, including his wife, \endgraf \hspace{5pt}Wu Shu-chen, his son, } \\
\hline
 and his son  \\
 is survived by his wife, Sybil MacKenzie and a son,  \\
 gave birth to a baby last week -- son  \\
\hline
\tablespace
\hline
 \bf (Puff Daddy, cuyos verdaderos nombre sea  \\
\it{ (Puff Daddy, whose real name is } \\
\hline%\hline
 (usually credited as {\it E1} \\
 (also known as Gero \#\#, real name  \\
 and (after changing his name to  \\
\hline
\tablespace
%\hline
%, Tian Tian, de \#\# a\~{n}os de edad, y su madre \\
%\it{, Tian Tian, \#\# years old, and his mother } \\
%\hline%\hline
% Gyllenhaal's parents -- screenwriter Naomi Foner and director  \\
% Brando's mother, actress Anna Kashfi, divorced  \\
% Cash, his mom was  \\
%\hline
%\tablespace
\hline
 \bf lleg\'{o} a la alfombra roja en compa\~{n}\'{i}a de su esposa, la \endgraf \hspace{5pt} actriz  Suzy Amis, casi una hora antes que su ex esposa, \\
\it{ arrived on the red carpet with his wife, \endgraf \hspace{5pt} actress  Suzy Amis, nearly an hour before his ex-wife , } \\
\hline%\hline
, who may or may not be having twins with husband  \\
, aged twenty, Kirk married \\
 went to elaborate lengths to keep his wedding to former \endgraf \hspace{5pt}supermodel \\
\hline
%\tablespace
%\hline
%\bf{{\it E1} , una firmes of estudios econ√≥micos lleven sede in {\it E2}}\\
%\it{{\it E1} an economic studies firm with headquarters in {\it E2}} \\
%\hline\hline
%{\it E2} offices of BP and {\it E1} \\
%{\it E2} , the Paris - based {\it E1} \\
%{\it E2} province , is the headquarters of {\it E1} \\
%\hline
\end{tabular}
\caption{Top English patterns for a Spanish query pattern encoded using the dictionary LSTM: For each Spanish query (English translation in italics), a list of English nearest neighbors. \label{tab:cross-lingual-relations}}
\end{center}
\end{table}


Our model jointly embeds KB relations together with English and Spanish text. We demonstrate that plausible textual patterns are embedded close to the KB relations they express. Table \ref{tab:top-tac-patterns} shows top scoring English and Spanish patterns given sample relations from our TAC KB.

\begin{table}[h]
\begin{center}
%\hspace*{-20pt}
\begin{tabular}{|p{7.8cm}|}
\hline
\textbf{per:sibling} \\
\hline
   \argOne, seg\'{u}n petici\'{o}n the primeros ministro, \endgraf \hspace{5pt} su hermano gemelo \argTwo  			\\ %\cline{3-3}
  \argOne, sea the principal favorito para esto oficina \endgraf \hspace{5pt}que tambi\'{e}n ambiciona su hermano \argTwo 	\\%\cline{3-3}
  \argOne, y su hermano gemelo, the primeros ministro \argTwo 	\\
\hline
  \argOne, for whose brother \argTwo  		\\%\cline{3-3}
  \argOne inherited his brother \argTwo 	\\%\cline{3-3}
  \argOne on saxophone and brother \argTwo 	\\
\hline\hline
%
\textbf{org:top\_members\_employees} \\
\hline
   \argTwo, presidente y director generales the \argOne  			\\%\cline{3-3}
   	\argTwo, presidente of the negocios especializada \argOne  	\\%\cline{3-3}
   	\argTwo (CIA), the director of the entidad, \argOne 	\\
\hline
 \argTwo, vice president and policy director of the \argOne  		\\%\cline{3-3}
 \argTwo, president of the German Soccer \argOne 	\\%\cline{3-3}
  \argTwo, president of the quasi-official \argOne 	\\
\hline\hline
%%
\textbf{per:alternate\_names} \\
\hline
   \argOne(como tambi\'{e}n son sabido para \argTwo 			\\%\cline{3-3}
   \argTwo-cuyos verdaderos nombre sea \argOne 	\\%\cline{3-3}
   	\argOne  tambi\'{e}n sabido como \argTwo 	\\
\hline
   \argOne aka \argTwo 		\\%\cline{3-3}
   \argOne, who also creates music under the pseudonym \argTwo 	\\%\cline{3-3}
   \argOne( of Modern Talking fame ) aka \argTwo  	\\
\hline\hline
%%
\textbf{per:cities\_of\_residence} \\
 \hline
  \argOne, poblado d\'{o}nde vive \argTwo 			\\%\cline{3-3}
   \argOne, una ciudadano naturalizado american\endgraf \hspace{5pt} y nacido in \argTwo 	\\%\cline{3-3}
   \argOne, que vive in \argTwo 	\\
\hline
   \argOne was born Jan. \# , \#\#\#\# in \argTwo 		\\%\cline{3-3}
   	\argOne was born on Monday in \argTwo 	\\%\cline{3-3}
   \argOne was born at Keighley in \argTwo 	\\
\hline
\end{tabular}
\caption{Top scoring patterns for both Spanish (top) and English (bottom) given query TAC relations. \label{tab:top-tac-patterns}}
\end{center}
\end{table}

\subsection {Implementation and Hyperparameters \label{sec:details}}
We performed a small grid search over learning rate {0.0001, 0.005, 0.001}, dropout {0.0, 0.1, 0.25, 0.5}, dimension {50, 100}, $\ell_2$ gradient clipping {1, 10, 50}, and epsilon {1e-8, 1e-6, 1e-4}. All models are trained for a maximum of 15 epochs. The CNN and LSTM both use 100d embeddings while USchema uses 50d. The CNN and LSTM both learned 100-dimensional word embeddings which were randomly initialized. Using pre-trained embeddings did not substantially affect the results. Entity pair embeddings for the baseline USchema model are randomly initialized. For the models with LSTM and CNN text encoders, entity pair embeddings are initialized using vectors from the baseline USchema model. This performs better than random initialization. We perform $\ell_2$ gradient clipping to 1 on all models. Universal Schema uses a batch size of 1024 while the CNN and LSTM use 128. All models are optimized using ADAM \citep{kingma2014adam} with $\epsilon=1e-8$, $\beta_1=0.9$, and $\beta_2=0.999$ with a learning rate of .001 for USchema and .0001 for CNN and LSTM. The CNN and LSTM also use dropout of 0.1 after the embedding layer.

\subsection{Details Concerning Cosine Similarity Computation \label{app:cosine}}
We measure the similarity between $r_{\text{text}}$ and $r_{\text{schema}}$ by computing the vectors' cosine similarity. However, such a distance is not well-defined, since the model was trained using inner products between entity vectors and relation vectors, not between two relation vectors. The US likelihood is invariant to invertible transformations of the latent coordinate system, since $\sigma\left( u_{s,o}^\top v_r \right) = \sigma\left( (A^\top u_{s,o})^\top A^{-1} v_r \right)$ for any invertible $A$. When taking inner products between two $v$ terms, however, the implicit $A^{-1}$ terms do not cancel out. We found that this issue can be minimized, and high quality predictive accuracy can be achieved, simply by using sufficient $\ell_2$ regularization to avoid implicitly learning an $A$ that substantially stretches the space.

\subsection{Data Pre-processing, Distant Supervision and Extraction Pipeline \label{sec:ds-el}}

We replace tokens occurring less than 5 times in the corpus with UNK and normalize all digits to \# (e.g. Oct-11-1988 becomes Oct-\#\#-\#\#\#\#).
For each sentence, we then extract all entity pairs and the text between them as surface patterns, ignoring patterns longer than 20 tokens.
This results in 48 million English `relations'. In Section~\ref{sec:norm}, we describe a technique for normalizing the surface patterns.
We filter out entity pairs that occurred less than 10 times in the data and extract the largest connected component in this entity co-occurrence graph.
This is necessary for the baseline US model, as otherwise learning decouples into independent problems per connected component.
Though the components are connected when using sentence encoders, we use only a single component to facilitate a fair comparison between modeling approaches.
We add the distant supervision training facts from the RelationFactory system, i.e. 352,236 entity-pair-relation tuples obtained from Freebase and high precision seed patterns.
The final training data contains a set of 3,980,164 (KB and openIE) facts made up of 549,760 unique entity pairs, 1,285,258 unique relations and 62,841 unique tokens.

We perform the same preprocessing on the Spanish data, resulting in 34 million raw surface patterns between entities.
We then filter patterns that never occur with an entity pair found in the English data.  This yields 860,502 Spanish patterns.
Our multilingual model is trained on a combination of these Spanish patterns, the English surface patterns, and the distant supervision data described above.
We learn word embeddings for 39,912 unique Spanish word types.
After parameter tying for translation pairs (Section \ref{sec:tie-words}),  there are 33,711 additional Spanish words not tied to English.


\subsection{Generation of Cross-Lingual Tied Word Types}
\label{sec:word-tying}
We follow the same procedure for generating translation pairs as \cite{mikolov2013}. First, we select the top 6000 words occurring in the lowercased Europarl dataset for each language and obtain a Google translation. We then filter duplicates and translations resulting in multi-word phrases. We also remove English past participles (ending in -ed) as we found the Google translation interprets these as adjectives (e.g.,  `she read the borrowed book' rather than `she borrowed the book') and much of the relational structure in language we seek to model is captured by verbs. This resulted in 6201 translation pairs that occurred in our text corpus. Though higher quality translation dictionaries would likely improve this technique, our experimental results show that such automatically generated dictionaries perform well.


\subsection{Open IE Pattern Normalization}
\label{sec:norm}
To improve US generalization, our US relations use log-shortened patterns where the middle tokens in patterns longer than five tokens are simplified. For each long pattern we take the first two tokens and last two tokens, and replace all $k$ remaining tokens with the number $\log k$. For example, the pattern {\bf Barack Obama} {\it is married to a person named} {\bf Michelle Obama} would be converted to: {\bf Barack Obama} {\it is married [1] person named} {\bf Michell Obama}. This shortening performs slightly better than whole patterns. LSTM and CNN variants use the entire sequence of tokens.
