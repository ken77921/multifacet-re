
\section{Introduction}
\label{introduction}
The goal of automatic knowledge base construction (AKBC) is to build a structured knowledge base (KB) of facts using a noisy corpus of raw text evidence, and perhaps an initial seed KB to be augmented~\citep{NELL,yago,freebase}. AKBC supports downstream reasoning at a high level about extracted entities and their relations, and thus has broad-reaching applications to a variety of domains.

One challenge in AKBC is aligning knowledge from a structured KB with a text corpus in order to perform supervised learning through \emph{distant supervision}. \emph{Universal schema}~\citep{limin} along with its extensions~\citep{yao2013universal,vector_pra,neelakantan2015compositional,logicmfnaacl15}, avoids alignment by jointly embedding KB relations, entities, and surface text patterns. This propagates information between KB annotation and corresponding textual evidence.

The above applications of universal schema express each text relation as a distinct item to be embedded. This harms its ability to generalize to inputs not precisely seen at training time. Recently,~\citet{toutanova2015representing} addressed this issue by embedding text patterns using a deep sentence encoder, which captures the compositional semantics of textual relations and allows for prediction on inputs never seen before. 

This paper further expands the coverage abilities of universal schema relation extraction by introducing techniques for forming predictions for new entities unseen in training and even for new domains with no associated annotation. In the extreme example of domain adaptation to a completely new language, we may have limited linguistic resources or labeled data such as treebanks, and only rarely a KB with adequate coverage. Our method performs multilingual transfer learning, providing a predictive model for a language with no coverage in an existing KB, by leveraging common representations for shared entities across text corpora. As depicted in Figure \ref{tab:multilingual-corpora}, we simply require that one language have an available KB of seed facts. We can further improve our models by tying a small set of word embeddings across languages using only simple knowledge about word-level translations, learning to embed semantically similar textual patterns from different languages into the same latent space. 

In extensive experiments on the TAC Knowledge Base Population (KBP) slot-filling benchmark we outperform the top 2013 system with an F1 score of 40.7 and perform relation extraction in Spanish with no labeled data or direct overlap between the Spanish training corpus and the training KB, demonstrating that our approach is well-suited for broad-coverage AKBC in low-resource languages and domains. Interestingly, joint training with Spanish improves English accuracy.


\begin{figure}[h!]
\begin{center}
\vspace{-1.9069cm}

\def\firstcircle{(0,0) circle (1.5cm)}
\def\secondcircle{(0:2cm) circle (1.5cm)}
\def\midline{[line width=1pt, dashed] node[label={[label distance=-3cm]-15:in KB}] {} node[label={[label distance=-3.5cm]15:not in KB}] {} (-3,0) -- (5,0)}
\begin{tikzpicture}

	\begin{scope}
	  \clip \firstcircle (-3,3) rectangle (3,3);
	  \fill[pattern=north west lines, pattern color=black!70] \firstcircle;
	\end{scope}
	\begin{scope}
	  \clip \secondcircle (0,0) rectangle (4,4);
	  \fill[pattern=north east lines, pattern color=black!70] \secondcircle;
	\end{scope}
	\begin{scope}
	  \clip \firstcircle;
	  \clip (0.5,0) rectangle (3,2);
	  \fill[white] \secondcircle;
	\end{scope}
	\draw \firstcircle node[above=1.5cm] {English};
	\draw \secondcircle node[above=1.5cm] {Low-resource};
	\draw \midline;

\end{tikzpicture}
\caption{Splitting the entities in a multilingual AKBC training set into parts. We only require that entities in the two corpora overlap. Remarkably, we can train a model for the low-resource language even if entities in the low-resource language do not occur in the KB. \label{tab:multilingual-corpora}}
\end{center}
\vspace{-.4cm}
\end{figure}

\section{Background \label{sec:background}}

AKBC extracts unary attributes of the form (\textit{subject}, \textit{attribute}), typed binary relations of the form (\textit{subject}, \textit{relation}, \textit{object}), or higher-order relations. We refer to subjects and objects as \textit{entities}. This work focuses solely on extracting binary relations, though many of our techniques generalize naturally to unary prediction. Generally, for example in Freebase~\citep{freebase}, higher-order relations are expressed in terms of collections of binary relations.

We now describe prior work on approaches to AKBC. They all aim to predict (\emph{s, r, o}) triples, but differ in terms of: (1) input data leveraged, (2) types of annotation required, (3) definition of relation label schema, and (4) whether they are capable of predicting relations for entities unseen in the training data. Note that all of these methods require pre-processing to detect entities, which may result in additional KB construction errors.

\subsection{Relation Extraction as Link Prediction \label{sec:prediction}}
A knowledge base is naturally described as a graph, in which entities are nodes and relations are labeled edges~\citep{yago,freebase}. In the case of \emph{knowledge graph completion}, the task is akin to link prediction, assuming an initial set of (\emph{s, r, o}) triples. See~\citet{nickel2015review} for a review. No accompanying text data is necessary, since links can be predicted using properties of the graph, such as transitivity. In order to generalize well, prediction is often posed as low-rank matrix or tensor factorization. A variety of model variants have been suggested, where the probability of a given edge existing depends on a multi-linear form~\citep{rescal,DBLP:journals/corr/Garcia-DuranBUG15,bishan,transe,wang2014knowledge,lin2015learning}, or non-linear interactions between $s$, $r$, and $o$~\citep{socherkb}.
Other approaches model the compositionality of multi-hop paths, typically for question answering \citep{bordes2014question,gu2015traversing,neelakantan2015compositional}.

\subsection{Relation Extraction as Sentence Classification
\label{seq:dist}}

Here, the training data consist of (1) a text corpus, and (2) a KB of seed facts with provenance, i.e. supporting evidence, in the corpus. Given individual an individual sentence, and pre-specified entities, a classifier predicts whether the sentence expresses a relation from a target schema. To train such a classifier, KB facts need to be aligned with supporting evidence in the text, but this is often challenging. For example, not all sentences containing Barack and Michelle Obama state that they are married. A variety of one-shot and iterative methods have addressed the alignment problem~\citep{bunescu2007learning,distant_supervision,riedel2010modeling,yao2010collective,hoffmann2011knowledge,surdeanu2012multi,min2013distant,zengdistant}.
An additional degree of freedom in these approaches is whether they classify individual sentences or predicting at the corpus level by aggregating information from all sentences containing a given pair of entities before prediction. The former approach is often preferable in practice, due to the simplicity of independently classifying individual sentences and the ease of associating each prediction with a provenance.
Prior work has applied deep learning to small-scale relation extraction problems, where functional relationships are detected between common nouns \citep{li2015tree,dos2015classifying}.
\citet{xu2015classifying} apply an LSTM to a parse path, while ~\citet{zengdistant} use a CNN on the raw text, with a special temporal pooling operation to separately embed the text around each entity.


\subsection{Open-Domain Relation Extraction
\label{sec:openIE}}
In the previous two approaches, prediction is carried out with respect to a fixed schema $R$ of possible relations $r$. This may overlook salient relations that are expressed in the text but do not occur in the schema. In response, \textit{open-domain} information extraction (OpenIE) lets the text speak for itself: $R$ contains all possible patterns of text occurring between entities $s$ and $o$~\citep{openie,etzioni2008open,resolver}. These are obtained by filtering and normalizing the raw text. The approach offers impressive coverage, avoids issues of distant supervision, and provides a useful exploratory tool. On the other hand, OpenIE predictions are difficult to use in downstream tasks that expect information from a fixed schema. 

Table~\ref{tab:patterns} provides examples of OpenIE patterns. The examples in row two and three illustrate relational contexts for which similarity is difficult to be captured by an OpenIE approach because of their syntactically complex constructions. This motivates the technique in Section~\ref{sec:encoder}, which uses a deep architecture applied to raw tokens, instead of rigid rules for normalizing text to obtain patterns.

\begin{table}[h!]
\small
\begin{center}
%\hspace{-1cm}
\begin{tabular}{|p{4.85cm}| p{2.37cm} | }
\hline
%Relation &
Sentence (context tokens italicized) & OpenIE pattern\\ \hline
%per:siblings &
{\bf Khan} \emph{'s younger sister,} {\bf Annapurna Devi}, who later married Shankar, developed into an equally accomplished master of the surbahar, but custom prevented her from performing in public. &  \argOne 's * sister \argTwo \\ \hline

%per:cities\_of\_residence &
A professor emeritus at Yale, {\bf Mandelbrot} \emph{was born in Poland but as a child moved with his family to} {\bf Paris} where he was educated. &  \argOne * moved with * family to \argTwo \\ \hline

%per:cities\_of\_residence &
{\bf Kissel} \emph{was born in Provo, Utah, but her family also lived in} {\bf Reno}. & \argOne * lived in \argTwo \\  \hline
\end{tabular}
\caption{Examples of sentences expressing relations. Context tokens (italicized) consist of the text occurring between entities (bold) in a sentence. OpenIE patterns are obtained by normalizing the context tokens using hand-coded rules. The top example expresses the per:siblings relation and the bottom two examples both express the per:cities\_of\_residence  relation. \label{tab:patterns}}
\end{center}
\vspace{-.4cm}
\end{table}

\subsection{Universal Schema}
When applying Universal Schema~\citep{limin} (USchema) to relation extraction, we combine the OpenIE and link-prediction perspectives.  By jointly modeling both OpenIE patterns and the elements of a target schema, the method captures broader relational structure than multi-class classification approaches that just model the target schema. Furthermore, the method avoids the distant supervision alignment difficulties of Section~\ref{seq:dist}. 

~\citet{limin} augment a knowledge graph from a seed KB with additional edges corresponding to OpenIE patterns observed in the corpus. Even if the user does not seek to predict these new edges, a joint model over all edges can exploit regularities of the OpenIE edges to improve modeling of the labels from the target schema. 

The data still consist of $(s,r,o)$ triples, which can be predicted using link-prediction techniques such as low-rank factorization.~\citet{limin} explore a variety of approximations to the 3-mode $(s,r,o)$ tensor. One such probabilistic model is:
\vspace{-.1cm}
\begin{equation}
\Prob \left((s,r,o)\right) = \sigma\left( u_{s,o}^\top v_r \right), \label{eq:US-prob}
\end{equation}
\vspace{-.5cm}

where  $\sigma()$ is a sigmoid function, $u_{s,o}$ is an embedding of the entity pair $(s,o)$, and $v_r$ is an embedding of the relation $r$, which may be an OpenIE pattern or a relation from the target schema. All of the exposition and results in this paper use this factorization, though many of the techniques we present later could be applied easily to the other factorizations described in ~\citet{limin}. Note that learning unique embeddings for OpenIE relations does not guarantee that similar patterns, such as the final two in Table~\ref{tab:patterns}, will be embedded similarly.

As with most of the techniques in Section~\ref{sec:prediction}, the data only consist of positive examples of edges. The absence of an annotated edge does not imply that the edge is false. In fact, we seek to predict some of these missing edges as true. ~\citet{limin} employ the Bayesian Personalized Ranking (BPR) approach of~\citet{rendle2009bpr}, which does not explicitly model unobserved edges as negative, but instead seeks to rank the probability of observed triples above unobserved triples.


Recently, \citet{toutanova2015representing} extended USchema to not learn individual pattern embeddings $v_r$, but instead to embed text patterns using a deep architecture applied to word tokens. This shares statistical strength between OpenIE patterns with similar words. We leverage this approach in Section~\ref{sec:encoder}. Additional work has modeled the regularities of multi-hop paths through knowledge graph augmented with text patterns~\citep{pra,pra_second,vector_pra,neelakantan2015compositional}.

\subsection{Multilingual Embeddings \label{sec:background-multilingual}}
Much work has been done on multilingual word embeddings.
Most of this work uses aligned sentences from the Europarl dataset~\citep{koehn2005europarl} to align word embeddings across languages~\citep{Gouws2015,luong2015bilingual,hermann2014multilingual}.
Others~\citep{mikolov2013,faruqui2014retrofitting} align separate single-language embedding models using a word-level dictionary.
\citet{mikolov2013} use translation pairs to learn a linear transform from one embedding space to another.


However, very little work exists on multilingual relation extraction. \citet{faruqui2015multilingual} perform multilingual OpenIE relation extraction by projecting all languages to English using Google translate. However, as explained in Section~\ref{sec:openIE} the OpenIE paradigm is not amenable to prediction within a fixed schema. Further, their approach does not generalize to low-resource languages where translation is unavailable -- while we use translation dictionaries to improve our results, our experiments demonstrate that our method is effective even without this resource.











