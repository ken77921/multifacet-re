
%Additionally, as explained in Section \ref{sec:uschema}, models trained to perform link prediction will often suffer from a lack of specificity resulting from learning only to `type check,' i.e. incorrectly predicting that Brad Pitt acted in the movie \emph{The Godfather}. Requiring text provinence makes it much more difficult to make this mistake, since it is highly unlikely that any text will support the above false relation. Embedding entity pairs, rather than embedding single entities then combining their encoded vectors to represent pairs, also helps to avoid the `type checking' problem, and given enough data has been shown to perform better than entity models \citep{toutanova2015representing}. We only experiment on embedding entity pairs as the distinction between entity and entity pairs is orthogonal to this work.


%Many papers evaluate on the popular FB15k dataset for link prediction in knowledge base completion. However, we do not find this interesting as it reduces to little more than type clustering. We are instead interested in learning high quality pattern encoders that, given providence, are able to accurately score the correlation \todo{better word} between patterns and patterns, and patterns and kb relations. In this situation, Brad Pitt would never be inferred to have acted in the Godfather unless there was provinence seen with that entity pair that scored highly with the acted\_in relation. Additionally, this allows us to evaluate unseen entities and address the coldstart problem.

%We choose to use entity pair vectors which, given adequate data, perform better than entity models see \citet{toutanova2015representing} test data with mentions. Although there are many models for kb embedding some using entity vectors and some using entity pair vectors, we do not investigate this distinction as it is orthogonal to our work.



\section{Task and System Description}

We focus on the TAC KBP slot-filling task. Much related work on embedding knowledge bases evaluates on the FB15k dataset \citep{transe,wang2014knowledge,lin2015learning,bishan,toutanova2015representing}. Here, relation extraction is posed as link prediction on a subset of Freebase.  This task does not capture the particular difficulties we address: (1) evaluation on entities and text unseen during training, and (2) zero-annotation learning of a predictor for a low-resource language.

Also, note both~\citet{toutanova2015representing} and~\citet{limin} explore the pros and cons of learning embeddings for entity pairs vs. separate embeddings for each entity. As this is orthogonal to our contributions, we only consider entity pair embeddings, which performed best in both works when given sufficient data.

%, which has received considerable attention in the NLP community since it focuses on a practical AKBC scenario characteristic of various real-world applications.

\subsection{TAC Slot-Filling Benchmark}

The aim of the TAC benchmark is to improve both coverage and quality of relation extraction evaluation compared to just checking the extracted facts against a knowledge base, which can be incomplete and where the provenances are not verified. In the slot-filling task, each system is given a set of paired query entities and relations or `slots' to fill, and the goal is to correctly fill as many slots as possible along with provenance from the corpus. For example, given the query entity/relation pair (\emph{Barack Obama, per:spouse}), the system should return the entity \emph{Michelle Obama} along with sentence(s) whose text expresses that relation. The answers returned by all participating teams, along with a human search (with timeout), are judged manually for correctness, i.e. whether the provenance specified by the system indeed expresses the relation in question.

 %Some slots, such as \emph{per:countries\_of\_residence} can be filled by multiple entities, whereas others, such as \emph{per:country\_of\_birth} can contain only one filler. The query entities are restricted to people (PER) and organizations (ORG) (rather than locations or other noun types, such as religion, which may fill query slots), and the 2013 English evaluation query set is made up of 50 PER entities and 50 ORG entities.

In addition to verifying our models on the 2013 and 2014 English slot-filling task, we evaluate our Spanish models on the 2012 TAC Spanish slot-filling evaluation. Because this TAC track was never officially run, the coverage of facts in the available annotation is very small, resulting in many correct predictions being marked incorrectly as precision errors. In response, we manually annotated all results returned by the models considered in Table~\ref{es-tac-table}. Precision and recall are calculated with respect to the union of the TAC annotation and our new labeling\footnote{Following \citet{surdeanu2012multi} we remove facts about undiscovered entities to correct for recall.}.


% 18 teams participated
% pooled and annotated - not just evaluated against (incomplete) knowledge base, and checked whether fact is actually expressed by text
%  \todo{Ben: something about the popularity of this task. Why  it is a good benchmark. }

\subsection{Retrieval Pipeline \label{sec:pipeline}}
Our retrieval pipeline first generates all valid slot filler candidates for each query entity and slot, based on entities extracted from the corpus using {\sc Factorie} ~\citep{mccallum09:factorie:} to perform tokenization, segmentation, and entity extraction. We perform entity linking by heuristically linking all entity mentions from our text corpora to a Freebase entity using anchor text in Wikipedia. Making use of the fact that most Freebase entries contain a link to the corresponding Wikipedia page, we link all entity mentions from our text corpora to a Freebase entity by the following process:
First, a set of candidate entities is obtained by following frequent link anchor text statistics.
We then select that candidate entity for which the cosine similarity between the respective Wikipedia and the sentence context of the mention is highest, and link to that entity if a threshold is exceeded.

An entity pair qualifies as a candidate prediction if it meets the type criteria for the slot.\footnote{Due to the difficulty of retrieval and entity detection, the maximum recall for predictions is limited. For this reason, \citet{surdeanu2012multi} restrict the evaluation to answer candidates returned by their system and effectively rescaling recall. We do not perform such a re-scaling in our English results in order to compare to other reported results. Our Spanish numbers are rescaled. All scores reflect the `anydoc' (relaxed) scoring to mitigate penalizing effects for systems not included in the evaluation pool.} The TAC 2013 English and Spanish newswire corpora each contain about 1 million newswire documents from 2009--2012. The document retrieval and entity matching components of our relation extraction pipeline are based on RelationFactory~\citep{roth2014relationfactory}, the top-ranked system of the 2013 English slot-filling task. We also use the English distantly supervised training data from this system, which aligns the TAC 2012 corpus to Freebase.
 More details on alignment are described in Appendix \ref{sec:ds-el}.

%Our maximum recall is significantly limited by our entity extraction pipeline: $\sim$60\% for English using the TAC `anydoc' (relaxed) scoring. \todo{Ben: juxtapose this with stanford scoring. explain what the SOA numbers are.} 

As discussed in Section~\ref{sec:non-comp}, models using a deep sentence encoder and using a pattern lookup table have complementary strengths and weaknesses. In response, we present results where we ensemble the outputs of the two models by simply taking the union of their individual outputs. Slightly higher results might be obtained through more sophisticated ensembling schemes. % We manually shift the models' thresholds to be more precision-biased, and take the union of the predictions returned by the two models. In contrast,~\citet{toutanova2015representing}, add the confidence scores of the systems and then apply a threshold. We found that this ensembling approach does not adequately account for the qualitative distinction in types of prediction that each technique can make accurately.



\subsection {Model Details \label{sec:models}}
All models are implemented in Torch (code publicly available\footnote{\url{https://github.com/patverga/torch-relation-extraction}}).
Models are tuned to maximize F1 on the 2012 TAC KBP slot-filling evaluation.
We additionally tune the thresholds of our pattern scorer on a per-relation basis to maximize F1 using 2012 TAC slot-filling for English and the 2012 Spanish slot-filling development set for Spanish.
As in~\citet{limin}, we train using the BPR loss of~\citet{rendle2009bpr}.
Our CNN is implemented as described in \citet{toutanova2015representing}, using width-3 convolutions, followed by tanh and max pool layers.
The LSTM uses a bi-directional architecture where the forward and backward representations of each hidden state are averaged, followed by max pooling over time.
See Section \ref{sec:details}

We also report results including an alternate names (AN) heuristic, which uses automatically-extracted rules to detect the TAC `alternate name' relation.
To achieve this, we collect frequent Wikipedia link anchor texts for each query entity.
If a high probability anchor text co-occurs with the canonical name of the query in the same document, we return the anchor text as a slot filler.


